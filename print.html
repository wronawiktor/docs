<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>go4clouds Kubernetes Tutorial</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="**go4clouds** is open source Kubernetes tutorial for application developers">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="kubernetes-introduction.html"><strong aria-hidden="true">2.</strong> Kubernetes introduction</a></li><li class="chapter-item expanded "><a href="cluster-overview.html"><strong aria-hidden="true">3.</strong> Cluster overview</a></li><li class="chapter-item expanded "><a href="pods-operations.html"><strong aria-hidden="true">4.</strong> Pods operations</a></li><li class="chapter-item expanded "><a href="advanced-pods.html"><strong aria-hidden="true">5.</strong> Advanced pods</a></li><li class="chapter-item expanded "><a href="application-deployment.html"><strong aria-hidden="true">6.</strong> Application deployment</a></li><li class="chapter-item expanded "><a href="using-configmaps-and-secrets.html"><strong aria-hidden="true">7.</strong> ConfigMaps and Secrets</a></li><li class="chapter-item expanded "><a href="persistent-volume.html"><strong aria-hidden="true">8.</strong> Persistent Volume</a></li><li class="chapter-item expanded "><a href="service-concept.html"><strong aria-hidden="true">9.</strong> Service concept</a></li><li class="chapter-item expanded "><a href="ingress-controller.html"><strong aria-hidden="true">10.</strong> Ingress controller</a></li><li class="chapter-item expanded "><a href="loadbalancer-installation.html"><strong aria-hidden="true">11.</strong> LoadBalancer installation</a></li><li class="chapter-item expanded "><a href="getting-highavailable.html"><strong aria-hidden="true">12.</strong> Getting High-Available</a></li><li class="chapter-item expanded "><a href="kubernetes-cni.html"><strong aria-hidden="true">13.</strong> Kubernetes CNI</a></li><li class="chapter-item expanded "><a href="network-policy.html"><strong aria-hidden="true">14.</strong> Network policy</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">go4clouds Kubernetes Tutorial</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/go4clouds/docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="https://go4clouds.github.io/docs/images/logo-horizontal-go4clouds.png" alt="go4clouds" /></p>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-kubernetes-introduction"><a class="header" href="#lab-exercises-for-kubernetes-introduction">Lab Exercises for Kubernetes introduction</a></h1>
<p>Visit following websites:</p>
<ul>
<li>https://kubernetes.io/docs/</li>
<li>https://kubernetes.io/blog/</li>
<li>https://github.com/kubernetes/</li>
<li>https://www.cncf.io/</li>
</ul>
<h2 id="exercise-0---login-and-prepare-lab-environment"><a class="header" href="#exercise-0---login-and-prepare-lab-environment">Exercise 0 - Login and prepare lab environment</a></h2>
<ul>
<li>Lab environment description </li>
</ul>
<pre><code>       virtualization: KVM with libvirt / virt-manager
       virtual network: 10.168.0.1/24
       virtual machines:
                        * master1 k8s-master1 - 10.168.0.101
                        * master2 k8s-master2 - 10.168.0.102
                        * master3 k8s-master3 - 10.168.0.103
                        * worker1 k8s-worker1 - 10.168.0.201
                        * worker2 k8s-worker2 - 10.168.0.202
</code></pre>
<ul>
<li>Start virtual machines, on k8s-host run:</li>
</ul>
<pre><code class="language-shell">virsh list --all
virsh start k8s-master1
virsh start k8s-master2
virsh start k8s-master3
virsh start k8s-worker1
virsh start k8s-worker2
</code></pre>
<p>or</p>
<pre><code class="language-shell">for SRV in k8s-master1 k8s-master2 k8s-master3 k8s-worker1 k8s-worker2; do virsh start $SRV; done
</code></pre>
<p>Note: All exercises please run <code>k8s-host</code></p>
<h2 id="exercise-1---install-kubernetes-client"><a class="header" href="#exercise-1---install-kubernetes-client">Exercise 1 - Install Kubernetes client</a></h2>
<ul>
<li>Install <code>kubectl</code> package</li>
</ul>
<pre><code class="language-shell">sudo apt-get install kubectl
kubectl version
</code></pre>
<h2 id="exercise-2---configure-kubeconfig-file"><a class="header" href="#exercise-2---configure-kubeconfig-file">Exercise 2 - Configure KUBECONFIG file</a></h2>
<ul>
<li>Download KUBECONfIG file</li>
</ul>
<pre><code class="language-shell">mkdir $HOME/.kube
scp root@master1:/etc/kubernetes/admin.conf $HOME/
cp $HOME/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<ul>
<li>Check cluster connection</li>
</ul>
<pre><code class="language-shell">kubectl cluster-info
</code></pre>
<p>Output</p>
<pre><code>Kubernetes control plane is running at https://k8smaster:6443
CoreDNS is running at https://k8smaster:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
<p>Alternative ways to use KUBECONFIG</p>
<pre><code class="language-shell">export KUBECONFIG=$HOME/admin.conf
kubectl cluster-info
</code></pre>
<p>or</p>
<pre><code class="language-shell">kubectl --kubeconfig=$HOME/admin.conf cluster-info
</code></pre>
<h2 id="exercise-3---check-and-configure-kubeconfig-config"><a class="header" href="#exercise-3---check-and-configure-kubeconfig-config">Exercise 3 - Check and configure KUBECONFIG config</a></h2>
<ul>
<li>Check current cluster config</li>
</ul>
<pre><code class="language-shell">kubectl config view
kubectl config get-clusters
kubectl config get-users
kubectl config get-contexts
kubectl config current-context
kubectl get pods
</code></pre>
<ul>
<li>Set admin context</li>
</ul>
<pre><code class="language-shell">kubectl config set-context kube-system-admin --cluster=kubernetes  --user=kubernetes-admin --namespace=kube-system
kubectl config get-contexts
kubectl config use-context kube-system-admin
kubectl get pods
</code></pre>
<ul>
<li>Use local context</li>
</ul>
<pre><code class="language-shell">kubectl config use-context kubernetes-admin@kubernetes
</code></pre>
<h2 id="exercise-4---setup-bash-auto-completion"><a class="header" href="#exercise-4---setup-bash-auto-completion">Exercise 4 - Setup bash auto-completion</a></h2>
<ul>
<li>Configure Kubernetes bash auto-completion</li>
</ul>
<pre><code class="language-shell">sudo apt-get install bash-completion
source &lt;(kubectl completion bash)
echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; $HOME/.bashrc
</code></pre>
<ul>
<li>Test bash auto-completion</li>
</ul>
<pre><code class="language-shell">kubec&lt;Tab&gt; cl&lt;Tab&gt;
</code></pre>
<h2 id="optional-1---setup-text-editor"><a class="header" href="#optional-1---setup-text-editor">Optional 1 - Setup text editor</a></h2>
<ul>
<li>Prepare text editor for editing Kubernetes yaml manifests</li>
</ul>
<pre><code class="language-shell">cat &lt;&lt; EOF &gt; ~/.vimrc
&quot; Sets tabstop to 2 for working with YAML
set ts=2
&quot; Sets softtabstop makes spaces feel like tabs
set sts=2
&quot; Sets the shift width to 2, making shift operations (&lt;&lt; or &gt;&gt;)               
set sw=2
&quot; Expands new tabs to spaces               
set expandtab
&quot; Convert all existing tabs to spaces     
retab                      
&quot; Enable syntax highlighting
syntax on               
&quot; For certain filetypes, enable automatic indenting
filetype indent plugin on 
&quot; Show column and line number   
set ruler                 
EOF
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-basic-commands"><a class="header" href="#lab-exercises-for-basic-commands">Lab Exercises for basic commands</a></h1>
<h2 id="exercise-0---checking-kubernets-cluster-status"><a class="header" href="#exercise-0---checking-kubernets-cluster-status">Exercise 0 - Checking Kubernets cluster status</a></h2>
<ul>
<li>Get Kubernetes cluster nodes list</li>
</ul>
<pre><code class="language-shell">kubectl get nodes
</code></pre>
<ul>
<li>Run once again previous command with new option <code>-o</code> and compare outputs</li>
</ul>
<pre><code class="language-shell">kubectl get nodes -o wide
</code></pre>
<ul>
<li>Check node description for k8s-master1</li>
</ul>
<pre><code class="language-shell">kubectl describe node k8s-master1
</code></pre>
<ul>
<li>Check node description for k8s-worker1</li>
</ul>
<pre><code class="language-shell">kubectl describe node k8s-worker1
</code></pre>
<h2 id="exercise-1---change-role-description-for-nodes"><a class="header" href="#exercise-1---change-role-description-for-nodes">Exercise 1 - Change role description for nodes</a></h2>
<ul>
<li>Show labels for nodes</li>
</ul>
<pre><code class="language-shell">kubectl get nodes --show-labels
</code></pre>
<ul>
<li>Change role name for worker nodes</li>
</ul>
<pre><code class="language-shell">kubectl label node k8s-worker1 node-role.kubernetes.io/worker=
kubectl label node k8s-worker2 node-role.kubernetes.io/worker=
</code></pre>
<p>or use some simple script</p>
<pre><code class="language-shell">for SRV in k8s-worker1 k8s-worker2; do
kubectl label node $SRV node-role.kubernetes.io/worker=
done
</code></pre>
<ul>
<li>Check nodes labels</li>
</ul>
<pre><code class="language-shell">kubectl get nodes --show-labels
</code></pre>
<ul>
<li>Remove node labels</li>
</ul>
<pre><code class="language-shell">kubectl label node k8s-worker1 node-role.kubernetes.io/worker-
kubectl label node k8s-worker2 node-role.kubernetes.io/worker-
</code></pre>
<h2 id="exercise-2---configure-node-taints"><a class="header" href="#exercise-2---configure-node-taints">Exercise 2 - Configure node taints</a></h2>
<ul>
<li>Check node taints for cluster</li>
</ul>
<pre><code class="language-shell">kubectl describe nodes | grep Taints
</code></pre>
<ul>
<li>Disable k8s-worker1 from scheduling</li>
</ul>
<pre><code class="language-shell">kubectl taint node k8s-worker1 node-role.kubernetes.io/worker=:NoSchedule
</code></pre>
<ul>
<li>Check node taints once again</li>
</ul>
<pre><code class="language-shell">kubectl describe nodes | grep Taints
</code></pre>
<ul>
<li>Remove node taint from k8s-worker1</li>
</ul>
<pre><code class="language-shell">kubectl taint node k8s-worker1 node-role.kubernetes.io/worker-
</code></pre>
<h2 id="exercise-3---annotate-nodes"><a class="header" href="#exercise-3---annotate-nodes">Exercise 3 - Annotate nodes</a></h2>
<ul>
<li>Add node annotation </li>
</ul>
<pre><code class="language-shell">kubectl annotate node k8s-master1 description=&quot;This is Kubernetes MASTER1 node!&quot;
</code></pre>
<ul>
<li>Check annotations section</li>
</ul>
<pre><code class="language-shell">kubectl describe node k8s-master1
</code></pre>
<ul>
<li>Remove annotation</li>
</ul>
<pre><code class="language-shell">kubectl annotate node k8s-master1 description-
</code></pre>
<h2 id="exercise-4---install-metric-server"><a class="header" href="#exercise-4---install-metric-server">Exercise 4 - Install Metric Server</a></h2>
<ul>
<li>Install Metric Server</li>
</ul>
<pre><code class="language-shell">kubectl apply -f metric-server.yaml
</code></pre>
<ul>
<li>Check Metric Server installation</li>
</ul>
<pre><code class="language-shell">kubectl get deploy -n kube-system metrics-server
</code></pre>
<ul>
<li>Check logs from Metric server</li>
</ul>
<pre><code class="language-shell">kubectl logs -n kube-system metrics-server-&lt;Tab&gt;
</code></pre>
<p>Output</p>
<pre><code>I0721 08:46:41.123090       1 secure_serving.go:197] Serving securely on [::]:443
I0721 08:46:41.124273       1 dynamic_serving_content.go:130] Starting serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key
I0721 08:46:41.124735       1 tlsconfig.go:240] Starting DynamicServingCertificateController
I0721 08:46:41.221247       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
I0721 08:46:41.221714       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0721 08:46:41.222167       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-pods"><a class="header" href="#lab-exercises-for-pods">Lab Exercises for Pods</a></h1>
<h2 id="exercise-0---check-list-of-namespaces"><a class="header" href="#exercise-0---check-list-of-namespaces">Exercise 0 - Check list of namespaces</a></h2>
<ul>
<li>List namespaces</li>
</ul>
<pre><code class="language-shell">kubectl get namespaces
</code></pre>
<h2 id="exercise-1---pods-basic-operations"><a class="header" href="#exercise-1---pods-basic-operations">Exercise 1 - Pods basic operations</a></h2>
<ul>
<li>List pods in <code>default</code> namespace</li>
</ul>
<pre><code class="language-shell">kubectl get pods
</code></pre>
<ul>
<li>List pods in <code>kube-system</code> namespace</li>
</ul>
<pre><code class="language-shell">kubectl get pods --namespace=kube-system
kubectl get pods -n kube-system
</code></pre>
<ul>
<li>List pods with <code>-o wide</code> option</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n kube-system -o wide
</code></pre>
<ul>
<li>Check pod status</li>
</ul>
<pre><code class="language-shell">kubectl describe pod -n kube-system kube-apiserver-k8s-master1
</code></pre>
<h2 id="exercise-2---create-new-pod"><a class="header" href="#exercise-2---create-new-pod">Exercise 2 - Create new pod</a></h2>
<ul>
<li>Create new pod in <code>default</code> namespace</li>
</ul>
<pre><code class="language-shell">kubectl run test1 --image=registry.k8s:5000/nginx
</code></pre>
<ul>
<li>Check pod status and wait until it will be <code>Running</code></li>
</ul>
<pre><code class="language-shell">kubectl get pod test1 -o wide -w
</code></pre>
<ul>
<li>Check logs test1</li>
</ul>
<pre><code class="language-shell">kubectl logs test1
</code></pre>
<ul>
<li>Connect to the terminal test1 pod</li>
</ul>
<pre><code class="language-shell">kubectl exec -ti test1 -- sh
# ls
# exit
</code></pre>
<ul>
<li>Cleanup test1 pod</li>
</ul>
<pre><code class="language-shell">kubectl delete pod test1
</code></pre>
<h2 id="exercise-3---generate-pod-template"><a class="header" href="#exercise-3---generate-pod-template">Exercise 3 - Generate pod template</a></h2>
<ul>
<li>Create new namespace and verify it</li>
</ul>
<pre><code class="language-shell">kubectl create namespace project1
kubectl get namespaces
</code></pre>
<ul>
<li>Generate pod template</li>
</ul>
<pre><code class="language-shell">kubectl run app1 -n project1 --image=debian --dry-run=client -o yaml
</code></pre>
<ul>
<li>Save pod template to file</li>
</ul>
<pre><code class="language-shell">kubectl run app1 -n project1 --image=registry.k8s:5000/ubuntu --dry-run=client -o yaml &gt; pod-app1.yaml
</code></pre>
<ul>
<li>Apply pod manifest to cluster</li>
</ul>
<pre><code class="language-shell">kubectl apply -f pod-app1.yaml
</code></pre>
<ul>
<li>Verify operation on cluster</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n project1 -o wide -w
</code></pre>
<ul>
<li>Cleanup environment </li>
</ul>
<pre><code class="language-shell">kubectl delete namespace project1
</code></pre>
<h2 id="exercise-4---start-debugging-container"><a class="header" href="#exercise-4---start-debugging-container">Exercise 4 - Start debugging container</a></h2>
<ul>
<li>Create new namespace</li>
</ul>
<pre><code class="language-shell">kubectl create namespace debug
</code></pre>
<ul>
<li>Start any Linux distro container</li>
</ul>
<pre><code class="language-shell">kubectl run test -n debug --image=registry.k8s:5000/ubuntu -- sleep 3600
</code></pre>
<ul>
<li>Wait until it will be <code>Running</code></li>
</ul>
<pre><code class="language-shell">kubectl get pods -n debug -o wide -w
</code></pre>
<ul>
<li>Connect to pod container terminal</li>
</ul>
<pre><code class="language-shell">kubectl exec -n debug -ti test -- bash
# apt-get update
# apt-get install tcpdump iproute2
# ip a
# tcpdump -i eth0
</code></pre>
<ul>
<li>Cleanup environment </li>
</ul>
<pre><code class="language-shell">kubectl delete namespace debug
</code></pre>
<h2 id="optional-0---write-your-own-example-application"><a class="header" href="#optional-0---write-your-own-example-application">Optional 0 - Write your own example application</a></h2>
<ul>
<li>Install <code>Docker</code> and <code>python3-pip</code> packages</li>
</ul>
<pre><code class="language-shell">sudo apt-get install docker.io python3-pip
</code></pre>
<ul>
<li>Add user <code>tux</code> to <code>docker</code> group</li>
</ul>
<pre><code class="language-shell">sudo usermod -a -G docker tux
</code></pre>
<p>Note: To apply changes please logout and login once again</p>
<ul>
<li>Create directory for application</li>
</ul>
<pre><code class="language-shell">mkdir myapp
cd myapp
</code></pre>
<ul>
<li>Write <code>myapp</code> python application</li>
</ul>
<pre><code class="language-shell">cat &gt; main.py &lt;&lt;EOF
from flask import Flask
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    return &quot;Hello from Python!&quot;

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0',port=8081)
EOF
</code></pre>
<ul>
<li>Prepate requirements file with <code>Flask</code></li>
</ul>
<pre><code class="language-shell">cat &gt; requirements.txt &lt;&lt;EOF
Flask
EOF
</code></pre>
<ul>
<li>Install python requirements</li>
</ul>
<pre><code class="language-shell">pip3 install -r requirements.txt
</code></pre>
<ul>
<li>Start application in terminal</li>
</ul>
<pre><code class="language-shell">python3 main.py
</code></pre>
<p>Output:</p>
<pre><code> * Serving Flask app 'main' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://172.16.4.253:8081/ (Press CTRL+C to quit)
</code></pre>
<ul>
<li>Open neww terminal and test application</li>
</ul>
<pre><code class="language-shell">curl http://0.0.0.0:8081
</code></pre>
<ul>
<li>Stop application and write <code>Dockerfile</code></li>
</ul>
<pre><code class="language-shell">cat &gt; Dockerfile &lt;&lt;EOF
FROM python:3.8

RUN mkdir /app
WORKDIR /app
ADD . /app/
RUN pip install -r requirements.txt

EXPOSE 8081
CMD [&quot;python&quot;, &quot;/app/main.py&quot;]
EOF
</code></pre>
<ul>
<li>Build container with application</li>
</ul>
<pre><code class="language-shell">docker build -f Dockerfile -t myapp:1.0 .
</code></pre>
<p>Output:</p>
<pre><code>Sending build context to Docker daemon  4.096kB
Step 1/7 : FROM python:3.8
3.8: Pulling from library/python
0bc3020d05f1: Pull complete 
a110e5871660: Pull complete 
83d3c0fa203a: Pull complete 
a8fd09c11b02: Pull complete 
14feb89c4a52: Pull complete 
70752631d778: Pull complete 
2273412836af: Pull complete 
5f59e94255df: Pull complete 
c95f8c6e2e3a: Pull complete 
Digest: sha256:83d2246349a8b864288bf9c0b193ce640b08889c14961b1925b47a9e5c9911b4
Status: Downloaded newer image for python:3.8
 ---&gt; b716d5a96fde
Step 2/7 : RUN mkdir /app
 ---&gt; Running in 6fda343c0718
Removing intermediate container 6fda343c0718
 ---&gt; 8557623c2a88
Step 3/7 : WORKDIR /app
 ---&gt; Running in 5e718f2b7448
Removing intermediate container 5e718f2b7448
 ---&gt; d0856bab751c
Step 4/7 : ADD . /app/
 ---&gt; 404b1b40ef8f
Step 5/7 : RUN pip install -r requirements.txt
 ---&gt; Running in e7d57084bfda
Collecting Flask
  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)
Collecting click&gt;=7.1.2
  Downloading click-8.0.1-py3-none-any.whl (97 kB)
Collecting Werkzeug&gt;=2.0
  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)
Collecting itsdangerous&gt;=2.0
  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)
Collecting Jinja2&gt;=3.0
  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)
Collecting MarkupSafe&gt;=2.0
  Downloading MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl (30 kB)
Installing collected packages: MarkupSafe, Werkzeug, Jinja2, itsdangerous, click, Flask
Successfully installed Flask-2.0.1 Jinja2-3.0.1 MarkupSafe-2.0.1 Werkzeug-2.0.1 click-8.0.1 itsdangerous-2.0.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Removing intermediate container e7d57084bfda
 ---&gt; 79909887eefe
Step 6/7 : EXPOSE 8081
 ---&gt; Running in 83423f716ff5
Removing intermediate container 83423f716ff5
 ---&gt; 51a186e4dc5b
Step 7/7 : CMD [&quot;python&quot;, &quot;/app/main.py&quot;]
 ---&gt; Running in 16e8e13f9a28
Removing intermediate container 16e8e13f9a28
 ---&gt; 14303156d868
Successfully built 14303156d868
Successfully tagged myapp:1.0
</code></pre>
<ul>
<li>Start locally <code>myapp</code> container and test it</li>
</ul>
<pre><code class="language-shell">docker run -p 8081:8081 myapp:1.0
</code></pre>
<p>On second terminal</p>
<pre><code class="language-shell">http://0.0.0.0:8081
</code></pre>
<ul>
<li>
<p>Register on on Docker hub https://hub.docker.com/</p>
</li>
<li>
<p>Login to your Docker Hub account</p>
</li>
</ul>
<pre><code class="language-shell">docker login
</code></pre>
<p>Output:</p>
<pre><code class="language-Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID  head over to https://hub.docker.com to create one.">Username: &lt;YOUR_ACCOUNT&gt;
Password: 
WARNING! Your password will be stored unencrypted in /home/tux/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
</code></pre>
<ul>
<li>Change tag name for <code>myapp</code> container</li>
</ul>
<pre><code class="language-shell">docker tag myapp:1.0 &lt;LOGIN&gt;/myapp:1.0
</code></pre>
<ul>
<li>Push <code>myapp</code> on Docker hub</li>
</ul>
<pre><code class="language-shell">docker push &lt;LOGIN&gt;/myapp:1.0
</code></pre>
<ul>
<li>Deploy <code>myapp</code> on Kubernetes</li>
</ul>
<pre><code class="language-shell">kubectl run myapp --image=&lt;LOIGN&gt;/myapp:1.0
</code></pre>
<ul>
<li>Check if it is running</li>
</ul>
<pre><code class="language-shell">kubectl get pod myapp -w -o wide
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-advanced-pods-operations"><a class="header" href="#lab-exercises-for-advanced-pods-operations">Lab Exercises for Advanced Pods Operations</a></h1>
<h2 id="exercise-0---specify-a-memory-request-and-a-memory-limit"><a class="header" href="#exercise-0---specify-a-memory-request-and-a-memory-limit">Exercise 0 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#specify-a-memory-request-and-a-memory-limit">Specify a memory request and a memory limit</a></a></h2>
<p>Create a namespace so that the resources you create in this exercise are isolated from the rest of your cluster</p>
<pre><code class="language-shell">kubectl create namespace mem-example
</code></pre>
<p>Create Pod that has one Container. The Container has a memory request of 100 MiB and a memory limit of 200 MiB. Here's the configuration file for the Pod:</p>
<pre><code class="language-shell">cat &gt; memory-request-limit.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: registry.k8s:5000/stress
    resources:
      limits:
        memory: &quot;200Mi&quot;
      requests:
        memory: &quot;100Mi&quot;
    command: [&quot;stress&quot;]
    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]
EOF
</code></pre>
<p>The args section in the configuration file provides arguments for the Container when it starts. The <code>--vm-bytes</code>, <code>150M</code> arguments tell the Container to attempt to allocate 150 MiB of memory.</p>
<p>Create the Pod:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit.yaml --namespace=mem-example
</code></pre>
<p>Verify that the Pod Container is running:</p>
<pre><code class="language-shell">kubectl get pod memory-demo --namespace=mem-example
</code></pre>
<p>View detailed information about the Pod:</p>
<pre><code class="language-shell">kubectl get pod memory-demo --output=yaml --namespace=mem-example
</code></pre>
<p>The output shows that the one Container in the Pod has a memory request of 100 MiB and a memory limit of 200 MiB.</p>
<pre><code>resources:
  limits:
    memory: 200Mi
  requests:
    memory: 100Mi
</code></pre>
<p>Run kubectl top to fetch the metrics for the pod:</p>
<pre><code class="language-shell">kubectl top pod memory-demo --namespace=mem-example
</code></pre>
<p>The output shows that the Pod is using about 162,900,000 bytes of memory, which is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the Pod's 200 MiB limit.</p>
<pre><code class="language-shell">NAME                        CPU(cores)   MEMORY(bytes)
memory-demo                 &lt;something&gt;  162856960
</code></pre>
<p>Delete your Pod:</p>
<pre><code class="language-shell">kubectl delete pod memory-demo --namespace=mem-example
</code></pre>
<h2 id="exercise-1---exceed-a-containers-memory-limit"><a class="header" href="#exercise-1---exceed-a-containers-memory-limit">Exercise 1 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/#exceed-a-container-s-memory-limit">Exceed a Container's memory limit</a></a></h2>
<p>A Container can exceed its memory request if the Node has memory available. But a Container is not allowed to use more than its memory limit. If a Container allocates more memory than its limit, the Container becomes a candidate for termination. If the Container continues to consume memory beyond its limit, the Container is terminated. If a terminated Container can be restarted, the kubelet restarts it, as with any other type of runtime failure.</p>
<p>In this exercise, you create a Pod that attempts to allocate more memory than its limit. Here is the configuration file for a Pod that has one Container with a memory request of 50 MiB and a memory limit of 100 MiB:</p>
<pre><code>cat &gt; memory-request-limit-2.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo-2
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-2-ctr
    image: registry.k8s:5000/stress
    resources:
      requests:
        memory: &quot;50Mi&quot;
      limits:
        memory: &quot;100Mi&quot;
    command: [&quot;stress&quot;]
    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;250M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]
EOF
</code></pre>
<p>In the args section of the configuration file, you can see that the Container will attempt to allocate 250 MiB of memory, which is well above the 100 MiB limit.</p>
<p>Create the Pod:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-2.yaml --namespace=mem-example
</code></pre>
<p>View detailed information about the Pod:</p>
<pre><code class="language-shell">kubectl get pod memory-demo-2 --namespace=mem-example
</code></pre>
<p>At this point, the Container might be running or killed. Repeat the preceding command until the Container is killed:</p>
<pre><code class="language-shell">NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   1          24s
</code></pre>
<p>Get a more detailed view of the Container status:</p>
<pre><code class="language-shell">kubectl get pod memory-demo-2 --output=yaml --namespace=mem-example
</code></pre>
<p>The output shows that the Container was killed because it is out of memory (OOM):</p>
<pre><code>lastState:
   terminated:
     containerID: docker://65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f
     exitCode: 137
     finishedAt: 2017-06-20T20:52:19Z
     reason: OOMKilled
     startedAt: null
</code></pre>
<p>The Container in this exercise can be restarted, so the kubelet restarts it. Repeat this command several times to see that the Container is repeatedly killed and restarted:</p>
<pre><code class="language-shell">kubectl get pod memory-demo-2 --namespace=mem-example
</code></pre>
<p>The output shows that the Container is killed, restarted, killed again, restarted again, and so on:</p>
<pre><code class="language-shell">kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   1          37s
</code></pre>
<pre><code class="language-shell">kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS    RESTARTS   AGE
memory-demo-2   1/1       Running   2          40s
</code></pre>
<p>View detailed information about the Pod history:</p>
<pre><code class="language-shell">kubectl describe pod memory-demo-2 --namespace=mem-example
</code></pre>
<p>The output shows that the Container starts and fails repeatedly:</p>
<pre><code>... Normal  Created   Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511
... Warning BackOff   Back-off restarting failed container
</code></pre>
<p>View detailed information about your cluster's Nodes:</p>
<pre><code class="language-shell">kubectl describe nodes
</code></pre>
<p>The output includes a record of the Container being killed because of an out-of-memory condition:</p>
<pre><code>Warning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child
</code></pre>
<p>Delete your Pod:</p>
<pre><code class="language-shell">kubectl delete pod memory-demo-2 --namespace=mem-example
</code></pre>
<p>Delete your namespace. This deletes all the Pods that you created for this task:</p>
<pre><code class="language-shell">kubectl delete namespace mem-example
</code></pre>
<h2 id="exercise-2---specify-a-cpu-request-and-a-cpu-limit"><a class="header" href="#exercise-2---specify-a-cpu-request-and-a-cpu-limit">Exercise 2 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#specify-a-cpu-request-and-a-cpu-limit">Specify a CPU request and a CPU limit</a></a></h2>
<p>Create a Namespace so that the resources you create in this exercise are isolated from the rest of your cluster.</p>
<pre><code class="language-shell">kubectl create namespace cpu-example
</code></pre>
<p>In this exercise, you create a Pod that has one container. The container has a request of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:</p>
<pre><code>cat &gt; cpu-request-limit.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
  namespace: cpu-example
spec:
  containers:
  - name: cpu-demo-ctr
    image: registry.k8s:5000/stress
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;0.5&quot;
    args:
    - -cpus
    - &quot;2&quot;
EOF
</code></pre>
<p>The args section of the configuration file provides arguments for the container when it starts. The -cpus &quot;2&quot; argument tells the Container to attempt to use 2 CPUs.</p>
<p>Create the Pod:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit.yaml --namespace=cpu-example
</code></pre>
<p>Verify that the Pod is running:</p>
<pre><code class="language-shell">kubectl get pod cpu-demo --namespace=cpu-example
</code></pre>
<p>View detailed information about the Pod:</p>
<pre><code class="language-shell">kubectl get pod cpu-demo --output=yaml --namespace=cpu-example
</code></pre>
<p>The output shows that the one container in the Pod has a CPU request of 500 milliCPU and a CPU limit of 1 CPU.</p>
<pre><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: 500m
</code></pre>
<p>Use kubectl top to fetch the metrics for the pod:</p>
<pre><code class="language-shell">kubectl top pod cpu-demo --namespace=cpu-example
</code></pre>
<p>This example output shows that the Pod is using 974 milliCPU, which is slightly less than the limit of 1 CPU specified in the Pod configuration.</p>
<pre><code>NAME                        CPU(cores)   MEMORY(bytes)
cpu-demo                    974m         &lt;something&gt;
</code></pre>
<h2 id="exercise-3---scheduling-pod-on-specific-node"><a class="header" href="#exercise-3---scheduling-pod-on-specific-node">Exercise 3 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/">Scheduling pod on specific node</a></a></h2>
<p>Add a label to a node
List the nodes in your cluster, along with their labels:</p>
<pre><code class="language-shell">kubectl get nodes --show-labels
</code></pre>
<p>The output is similar to this:</p>
<pre><code>NAME          STATUS   ROLES                  AGE     VERSION   LABELS
k8s-master1   Ready    control-plane,master   7h58m   v1.20.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
k8s-master2   Ready    control-plane,master   7h56m   v1.20.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master2,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
k8s-master3   Ready    control-plane,master   7h55m   v1.20.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master3,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
k8s-worker1   Ready    &lt;none&gt;                 7h54m   v1.20.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker1,kubernetes.io/os=linux
k8s-worker2   Ready    &lt;none&gt;                 7h54m   v1.20.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker2,kubernetes.io/os=linux
</code></pre>
<p>Chose one of your nodes, and add a label to it:</p>
<pre><code class="language-shell">kubectl label nodes k8s-worker1 disktype=ssd
</code></pre>
<p>where k8s-worker1 is the name of your chosen node.</p>
<p>Verify that your chosen node has a <code>disktype=ssd</code> label:</p>
<pre><code class="language-shell">kubectl get nodes --show-labels
</code></pre>
<p>The output is similar to this:</p>
<pre><code>NAME          STATUS    ROLES    AGE     VERSION        LABELS
...
k8s-worker1   Ready     &lt;none&gt;   1d      v1.20.9        ...,disktype=ssd,kubernetes.io/hostname=k8s-worker1
k8s-worker2   Ready     &lt;none&gt;   1d      v1.20.9        ...,kubernetes.io/hostname=k8s-worker2
</code></pre>
<p>In the preceding output, you can see that the worker0 node has a <code>disktype=ssd</code> label.</p>
<p>Create a pod that gets scheduled to your chosen node
This pod configuration file describes a pod that has a node selector, disktype: ssd. This means that the pod will get scheduled on a node that has a disktype=ssd label.</p>
<pre><code>cat &gt; pod-nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: registry.k8s:5000/nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
EOF
</code></pre>
<p>Use the configuration file to create a pod that will get scheduled on your chosen node:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml
</code></pre>
<p>Verify that the pod is running on your chosen node:</p>
<pre><code class="language-shell">kubectl get pods --output=wide
</code></pre>
<p>The output is similar to this:</p>
<pre><code>NAME      READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS
nginx     1/1     Running   0          4d2h   192.168.126.10   k8s-worker1   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Chose one of your nodes, and add a label to it:</p>
<pre><code class="language-shell">kubectl label nodes &lt;your-node-name&gt; disktype=ssd
</code></pre>
<p>where <your-node-name> is the name of your chosen node.</p>
<p>Verify that your chosen node has a disktype=ssd label:</p>
<pre><code class="language-shell">kubectl get nodes --show-labels
</code></pre>
<p>The output is similar to this:</p>
<pre><code>NAME          STATUS    ROLES    AGE     VERSION        LABELS
k8s-worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=k8s-worker1
k8s-worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=k8s-worker2
</code></pre>
<p>In the preceding output, you can see that the worker0 node has a <code>disktype=ssd</code> label.</p>
<p>Create a pod that gets scheduled to your chosen node
This pod configuration file describes a pod that has a node selector, disktype: ssd. This means that the pod will get scheduled on a node that has a disktype=ssd label.</p>
<pre><code>cat &gt; pod-nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: registry.k8s:5000/nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
EOF
</code></pre>
<p>Use the configuration file to create a pod that will get scheduled on your chosen node:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml
</code></pre>
<p>Verify that the pod is running on your chosen node:</p>
<pre><code class="language-shell">kubectl get pods --output=wide
</code></pre>
<p>The output is similar to this:</p>
<pre><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   k8s-worker1
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-application-deployment"><a class="header" href="#lab-exercises-for-application-deployment">Lab Exercises for Application Deployment</a></h1>
<h2 id="exercise-0---run-application-using-a-deployment"><a class="header" href="#exercise-0---run-application-using-a-deployment">Exercise 0 - <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">Run Application Using a Deployment</a></a></h2>
<p>Creating and exploring an nginx deployment
You can run an application by creating a Kubernetes Deployment object, and you can describe a Deployment in a YAML file. For example, this YAML file describes a Deployment that runs the <code>nginx:1.14.2</code> Docker image:</p>
<pre><code>cat &gt; deployment.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: registry.k8s:5000/nginx:1.14.2
        ports:
        - containerPort: 80
EOF
</code></pre>
<p>Create a Deployment based on the YAML file:</p>
<pre><code class="language-shell">kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</code></pre>
<p>Display information about the Deployment:</p>
<pre><code class="language-shell">kubectl describe deployment nginx-deployment
</code></pre>
<p>The output is similar to this:</p>
<pre><code> Name:     nginx-deployment
 Namespace:    default
 CreationTimestamp:  Tue, 30 Aug 2016 18:11:37 -0700
 Labels:     app=nginx
 Annotations:    deployment.kubernetes.io/revision=1
 Selector:   app=nginx
 Replicas:   2 desired | 2 updated | 2 total | 2 available | 0 unavailable
 StrategyType:   RollingUpdate
 MinReadySeconds:  0
 RollingUpdateStrategy:  1 max unavailable, 1 max surge
 Pod Template:
   Labels:       app=nginx
   Containers:
    nginx:
     Image:              nginx:1.14.2
     Port:               80/TCP
     Environment:        &lt;none&gt;
     Mounts:             &lt;none&gt;
   Volumes:              &lt;none&gt;
 Conditions:
   Type          Status  Reason
   ----          ------  ------
   Available     True    MinimumReplicasAvailable
   Progressing   True    NewReplicaSetAvailable
 OldReplicaSets:   &lt;none&gt;
 NewReplicaSet:    nginx-deployment-1771418926 (2/2 replicas created)
 No events.
</code></pre>
<p>List the Pods created by the deployment:</p>
<pre><code class="language-shell"> kubectl get pods -l app=nginx
</code></pre>
<p>The output is similar to this:</p>
<pre><code> NAME                                READY     STATUS    RESTARTS   AGE
 nginx-deployment-1771418926-7o5ns   1/1       Running   0          16h
 nginx-deployment-1771418926-r18az   1/1       Running   0          16h
</code></pre>
<p>Display information about a Pod:</p>
<pre><code class="language-shell"> kubectl describe pod &lt;pod-name&gt;
</code></pre>
<p>where <pod-name> is the name of one of your Pods.</p>
<p>Updating the deployment
You can update the deployment by applying a new YAML file. This YAML file specifies that the deployment should be updated to use nginx 1.16.1.</p>
<pre><code>cat &gt; deployment-update.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: registry.k8s:5000/nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1
        ports:
        - containerPort: 80
EOF
</code></pre>
<p>Apply the new YAML file:</p>
<pre><code class="language-shell">  kubectl apply -f https://k8s.io/examples/application/deployment-update.yaml
</code></pre>
<p>Watch the deployment create pods with new names and delete the old pods:</p>
<pre><code class="language-shell">  kubectl get pods -l app=nginx
</code></pre>
<p>Scaling the application by increasing the replica count
You can increase the number of Pods in your Deployment by applying a new YAML file. This YAML file sets replicas to 4, which specifies that the Deployment should have four Pods:</p>
<pre><code>cat &gt; deployment-scale.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 4 # Update the replicas from 2 to 4
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: registry.k8s:5000/nginx:1.14.2
        ports:
        - containerPort: 80
EOF
</code></pre>
<p>Apply the new YAML file:</p>
<pre><code class="language-shell"> kubectl apply -f https://k8s.io/examples/application/deployment-scale.yaml
</code></pre>
<p>Verify that the Deployment has four Pods:</p>
<pre><code class="language-shell"> kubectl get pods -l app=nginx
</code></pre>
<p>The output is similar to this:</p>
<pre><code> NAME                               READY     STATUS    RESTARTS   AGE
 nginx-deployment-148880595-4zdqq   1/1       Running   0          25s
 nginx-deployment-148880595-6zgi1   1/1       Running   0          25s
 nginx-deployment-148880595-fxcez   1/1       Running   0          2m
 nginx-deployment-148880595-rwovn   1/1       Running   0          2m
</code></pre>
<p>Delete the deployment by name:</p>
<pre><code class="language-shell">kubectl delete deployment nginx-deployment
</code></pre>
<h2 id="exercise-1---testing-rolling-update-deployment"><a class="header" href="#exercise-1---testing-rolling-update-deployment">Exercise 1 - Testing rolling update deployment</a></h2>
<ul>
<li>Create test deployment</li>
</ul>
<pre><code class="language-shell">kubectl create deployment test --image=registry.k8s:5000/nginx:1.14.2
</code></pre>
<ul>
<li>Check deployment status</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods
kubectl rollout history deployment test
</code></pre>
<ul>
<li>Change number of replicas</li>
</ul>
<pre><code class="language-shell">kubectl scale deployment test --replicas=3 --record
</code></pre>
<ul>
<li>Check deployment status</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods
</code></pre>
<ul>
<li>Update deployment</li>
</ul>
<pre><code class="language-shell">kubectl set image deployment test nginx=registry.k8s:5000/nginx:1.16.1 --record
</code></pre>
<ul>
<li>Check deployment status </li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods
</code></pre>
<ul>
<li>Check deployment rollout history</li>
</ul>
<pre><code class="language-shell">kubectl rollout history --revision=2 deployment test
</code></pre>
<ul>
<li>Change resource limits </li>
</ul>
<pre><code class="language-shell">kubectl set resources deployment test --containers=nginx --limits=cpu=200m,memory=512Mi --record
</code></pre>
<ul>
<li>Braking deployment </li>
</ul>
<pre><code class="language-shell">kubectl rollout history deployment test
kubectl set image deployment test nginx=nginx:1.173 --record 
</code></pre>
<ul>
<li>Check the status</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods
kubectl rollout status deployment test
</code></pre>
<ul>
<li>Deployment rollback</li>
</ul>
<pre><code class="language-shell">kubectl rollout undo deployment test
</code></pre>
<h2 id="exercise-2---enable-autoscaling"><a class="header" href="#exercise-2---enable-autoscaling">Exercise 2 - Enable autoscaling</a></h2>
<ul>
<li>Update nginx deployment and add resources requests CPU <code>200m</code></li>
</ul>
<pre><code class="language-shell">kubectl patch deployment test --type='json' -p='[{&quot;op&quot;:&quot;add&quot;, &quot;path&quot;:&quot;/spec/template/spec/containers/0/resources/requests&quot;, &quot;value&quot;:{&quot;cpu&quot;:&quot;200m&quot;}}]'
</code></pre>
<ul>
<li>Enable Horizontal Pod Autoscaler for <code>test</code> deployment</li>
</ul>
<pre><code class="language-shell">kubectl autoscale deployment test --min=1 --max=5 --cpu-percent=60
</code></pre>
<ul>
<li>Check HPA staus</li>
</ul>
<pre><code class="language-shell">kubectl get hpa nginx
</code></pre>
<p>Output:</p>
<pre><code>NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/nginx   0%/60%    1         5         1          36m
</code></pre>
<ul>
<li>Check HPA event logs</li>
</ul>
<pre><code class="language-shell">kubectl describe hpa nginx
</code></pre>
<ul>
<li>Clean up</li>
</ul>
<pre><code class="language-shell">kubectl delete deployment test
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-configmaps-and-secrets"><a class="header" href="#lab-exercises-for-configmaps-and-secrets">Lab Exercises for ConfigMaps and Secrets</a></h1>
<h2 id="exercise-0---create-configmaps-from-files"><a class="header" href="#exercise-0---create-configmaps-from-files">Exercise 0 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files">Create ConfigMaps from files</a></a></h2>
<p>Create the local directory</p>
<pre><code class="language-shell">mkdir -p configure-pod-container/configmap/
</code></pre>
<p>Download the sample files into <code>configure-pod-container/configmap/</code> directory</p>
<pre><code class="language-shell">wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties
wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties
</code></pre>
<p>You can use kubectl create configmap to create a ConfigMap from an individual file, or from multiple files.</p>
<pre><code class="language-shell">kubectl create configmap game-config --from-file=configure-pod-container/configmap/game.properties
</code></pre>
<p>Check new ConfigMap properties</p>
<pre><code class="language-shell">kubectl describe configmaps game-config
</code></pre>
<p>You can pass in the <code>--from-file</code> argument multiple times to create a ConfigMap from multiple data sources.</p>
<pre><code class="language-shell">kubectl create configmap game-config-multi --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties
</code></pre>
<p>You can display details of the game-config-multi ConfigMap using the following command:</p>
<pre><code class="language-shell">kubectl describe configmaps game-config-multi
</code></pre>
<p>You can use kubectl create configmap with the <code>--from-literal</code> argument to define a literal value from the command line:</p>
<pre><code class="language-shell">kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm
</code></pre>
<p>You can pass in multiple key-value pairs. Each pair provided on the command line is represented as a separate entry in the data section of the ConfigMap.</p>
<pre><code class="language-shell">kubectl get configmaps special-config -o yaml
</code></pre>
<h2 id="exercise-1---define-container-environment-variables-using-configmap-data"><a class="header" href="#exercise-1---define-container-environment-variables-using-configmap-data">Exercise 1 - <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-a-container-environment-variable-with-data-from-a-single-configmap">Define container environment variables using ConfigMap data</a></a></h2>
<p>Define an environment variable as a key-value pair in a ConfigMap:</p>
<pre><code class="language-shell">kubectl create configmap special-config --from-literal=special.how=very
</code></pre>
<p>Assign the special.how value defined in the ConfigMap to the SPECIAL_LEVEL_KEY environment variable in the Pod specification.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]
      env:
        # Define the environment variable
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
              name: special-config
              # Specify the key associated with the value
              key: special.how
  restartPolicy: Never
</code></pre>
<p>Create the Pod:</p>
<pre><code class="language-shell">kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml
</code></pre>
<p>Now, the Pod's output includes environment variable <code>SPECIAL_LEVEL_KEY=very</code></p>
<h2 id="exercise-2---create-secret-object"><a class="header" href="#exercise-2---create-secret-object">Exercise 2 - Create Secret object</a></h2>
<ul>
<li>Create a secret named mysecret that has the following key=value pair</li>
</ul>
<pre><code>dbusername = MyDatabaseUser
dbpassword = MyDatabasePassword
</code></pre>
<pre><code class="language-shell">kubectl create secret generic mysecret --from-literal=dbusername=&quot;MyDatabaseUser&quot; --from-literal=dbpassword=&quot;MyDatabasePassword&quot;
</code></pre>
<ul>
<li>Create a pod of your choice, such as nginx. Configure this Pod so that the underlying container has the the following environment variables set:</li>
</ul>
<p>DBUSER from secret key dbusername
DBPASS from secret key dbpassword</p>
<pre><code class="language-shell">apiVersion: v1
kind: Pod
metadata:
  name: nginx-secret
spec:
  containers:
  - name: nginx-secret
    image: registry.k8s:5000/nginx
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]
    env:
      - name: dbuser
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: dbuser
      - name: dbpassword
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: dbpassword
  restartPolicy: Never
</code></pre>
<p>Validate configuration with:</p>
<pre><code class="language-shell">kubectl logs nginx-secret | grep db
</code></pre>
<p>Output should be:</p>
<pre><code>dbuser=MyDatabaseUsername
dbpassword=MyDatabasePassword
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-persistent-volume"><a class="header" href="#lab-exercises-for-persistent-volume">Lab Exercises for Persistent volume</a></h1>
<h2 id="exercise-0---create-persistent-nfs-volume"><a class="header" href="#exercise-0---create-persistent-nfs-volume">Exercise 0 - Create Persistent NFS Volume</a></h2>
<ul>
<li>Deploy and configure NFS server </li>
</ul>
<pre><code class="language-shell">sudo apt-get install -y nfs-kernel-server
</code></pre>
<ul>
<li>Create NFS export directory and set proper perimissions</li>
</ul>
<pre><code class="language-shell">sudo mkdir /srv/share
sudo chmod 1777 /srv/share
echo &quot;Hello World&quot; &gt; /srv/share/index.html
</code></pre>
<ul>
<li>Export NFS directory to Kubernetes cluster nodes</li>
</ul>
<pre><code class="language-shell">sudo vim /etc/exports
/srv/share/ *(rw,sync,no_root_squash,subtree_check)
</code></pre>
<ul>
<li>Check /etc/exports</li>
</ul>
<pre><code class="language-shell">sudo exportfs -ra
</code></pre>
<ul>
<li>On all Kubernetes nodes install <code>nfs-common</code> package</li>
</ul>
<pre><code class="language-shell">for SRV in master1 master2 master3 worker1 worker2; do
ssh root@$SRV apt-get install nfs-common -y
done
</code></pre>
<ul>
<li>Check if nodes see shares</li>
</ul>
<pre><code class="language-shell">for SRV in master1 master2 master3 worker1 worker2; do
ssh root@$SRV showmount -e 10.168.0.1
done
</code></pre>
<ul>
<li>Prepare PersistentVolume YAML manifest</li>
</ul>
<pre><code class="language-shell">cat &gt; pv-nfs.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /srv/share
    server: 10.168.0.1
    readOnly: false
EOF
</code></pre>
<ul>
<li>Apply PersistentVolume YAML manifest</li>
</ul>
<pre><code class="language-shell">kubectl apply -f pv-nfs.yaml
</code></pre>
<ul>
<li>Check list PersistentVolumes</li>
</ul>
<pre><code class="language-shell">kubectl get pv
</code></pre>
<ul>
<li>To use PersisentVolume it is required create Persistent Volume Claim YAML manifest</li>
</ul>
<pre><code class="language-shell">cat &gt; pvc-nfs.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF
</code></pre>
<ul>
<li>Apply Persistent Volume Claim </li>
</ul>
<pre><code class="language-shell">kubectl apply -f pvc-nfs.yaml
</code></pre>
<ul>
<li>Get list of Persistent Volume Claims</li>
</ul>
<pre><code class="language-shell">kubectl get pvc
</code></pre>
<ul>
<li>Let's create new pod which will use the PVC</li>
</ul>
<pre><code>cat &gt; app-nfs.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: app-nfs
  name: app-nfs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-nfs
  template:
    metadata:
      labels:
        app: app-nfs
    spec:
      containers:
      - image: registry.k8s:5000/nginx
        name: nginx
        volumeMounts:
          - mountPath: &quot;/usr/share/nginx/html&quot;
            name: vol-nfs
      volumes:
        - name: vol-nfs
          persistentVolumeClaim:
            claimName: pvc-nfs
</code></pre>
<ul>
<li>Start application with nfs volume mounted</li>
</ul>
<pre><code class="language-shell">kubectl apply -f app-nfs.yaml
</code></pre>
<ul>
<li>Check pod status</li>
</ul>
<pre><code class="language-shell">kubectl get pods
</code></pre>
<ul>
<li>Verify PersistentVolume and PersisentVolumeClaim</li>
</ul>
<pre><code class="language-shell">kubectl get pvc,pv
</code></pre>
<ul>
<li>Get pod description </li>
</ul>
<pre><code class="language-shell">kubectl describe pod app-nfs-&lt;Tab&gt;
</code></pre>
<ul>
<li>Check application response</li>
</ul>
<pre><code class="language-shell">curl http://POD_IP
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-service-concept"><a class="header" href="#lab-exercises-for-service-concept">Lab Exercises for Service Concept</a></h1>
<p>Notes: <a href="https://docs.projectcalico.org/about/about-kubernetes-services">Kubernetes services</a></p>
<h2 id="exercise-0---exposing-application-deployment"><a class="header" href="#exercise-0---exposing-application-deployment">Exercise 0 - Exposing application deployment</a></h2>
<ul>
<li>Create new namespace</li>
</ul>
<pre><code class="language-shell">kubectl create namespace frontend
</code></pre>
<ul>
<li>Deploy example application</li>
</ul>
<pre><code class="language-shell">kubectl create deployment web --image=nginx -n frontend
</code></pre>
<ul>
<li>Check deployment status</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods -n frontend -o wide
</code></pre>
<ul>
<li>Scale application </li>
</ul>
<pre><code class="language-shell">kubectl scale deployment -n frontend web --replicas=2
</code></pre>
<ul>
<li>Expose application deployment with service</li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n frontend web --port=80
</code></pre>
<ul>
<li>Check service status</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n frontend web
</code></pre>
<ul>
<li>Try to connect application using <code>ClusterIP</code></li>
</ul>
<pre><code class="language-shell">curl http://&lt;ClusterIP&gt;
</code></pre>
<ul>
<li>Try to connect application from cluster nodes</li>
</ul>
<pre><code class="language-shell">for SRV in master1 master2 master3 worker1 worker2; do
ssh $SRV curl http://&lt;ClusterIP&gt;
done
</code></pre>
<ul>
<li>Display service endpoints</li>
</ul>
<pre><code class="language-shell">kubectl get endpoints -n frontend web
</code></pre>
<ul>
<li>Scale up application once again</li>
</ul>
<pre><code class="language-shell">kubectl scale deployment -n frontend web --replicas=5
</code></pre>
<ul>
<li>Check list of endpoints </li>
</ul>
<pre><code class="language-shell">kubectl get endpoints -n frontend web
</code></pre>
<ul>
<li>Show deployment properties</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods -n frontend -o wide
</code></pre>
<ul>
<li>Show deployment pods labels</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n frontend --show-labels
NAME                  READY   STATUS    RESTARTS   AGE     LABELS
web-96d5df5c8-7qqlb   1/1     Running   0          17s     app=web,pod-template-hash=96d5df5c8
web-96d5df5c8-b9kng   1/1     Running   0          6m42s   app=web,pod-template-hash=96d5df5c8
web-96d5df5c8-hwk9z   1/1     Running   0          17s     app=web,pod-template-hash=96d5df5c8
web-96d5df5c8-pzclb   1/1     Running   0          11m     app=web,pod-template-hash=96d5df5c8
web-96d5df5c8-qr26w   1/1     Running   0          6m42s   app=web,pod-template-hash=96d5df5c8
</code></pre>
<ul>
<li>Remove pod <code>app=web</code> label</li>
</ul>
<pre><code class="language-shell">kubectl label pod -n frontend web-&lt;Tab&gt;-&lt;Tab&gt; app-
</code></pre>
<ul>
<li>Check once again pod lists</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n frontend --show-labels
</code></pre>
<ul>
<li>List pods only with label <code>app=web</code></li>
</ul>
<pre><code class="language-shell">kubectl get pods -n frontend --selector=&quot;app=web&quot; -o wide
</code></pre>
<ul>
<li>Remove service object</li>
</ul>
<pre><code class="language-shell">kubectl delete service -n frontend web
</code></pre>
<ul>
<li>Create once again deployment service but with another mode</li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n frontend web --type=NodePort --port=80
</code></pre>
<ul>
<li>Check service status</li>
</ul>
<pre><code>kubectl get svc -n frontend web
</code></pre>
<p>Output should looks similar to:</p>
<pre><code>NAME   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
web    NodePort   10.96.208.39   &lt;none&gt;        80:30237/TCP   10s
</code></pre>
<ul>
<li>Use <code>ClusterIP</code> to connect to service from outside cluster</li>
</ul>
<pre><code class="language-shell">NODEPORT=`kubectl get svc -n frontend web -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot;`
echo &quot;NodePort for web service is $NODEPORT&quot;
curl http://k8s-master1:$NODEPORT
</code></pre>
<p>or on all nodes</p>
<pre><code class="language-shell">NODEPORT=`kubectl get svc -n frontend web -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot;`
for SRV in k8s-master1 k8s-master2 k8s-master3 k8s-worker1 k8s-worker2; do
curl http://$SRV:$NODEPORT
done
</code></pre>
<ul>
<li>Let's do clean up</li>
</ul>
<pre><code class="language-shell">kubectl delete namespace frontend
</code></pre>
<h2 id="exercise-1---use-port-forwarding-to-access-applications"><a class="header" href="#exercise-1---use-port-forwarding-to-access-applications">Exercise 1 - <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#creating-mongodb-deployment-and-service">Use Port Forwarding to Access Applications</a></a></h2>
<p>Create a Deployment that runs MongoDB:</p>
<pre><code class="language-shell">cat &gt; mongo-deployment.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: mongo
  name: mongo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - image: mongo
        name: mongo
        ports:
        - name: mongodbport
          containerPort: 27017
          protocol: TCP
EOF
</code></pre>
<p>Apply YAML manifest</p>
<pre><code class="language-shell">kubectl apply -f mongo-deployment.yaml
</code></pre>
<p>The output of a successful command verifies that the deployment was created:</p>
<pre><code>deployment.apps/mongo created
</code></pre>
<p>View the pod status to check that it is ready:</p>
<pre><code class="language-shell">kubectl get pods
</code></pre>
<p>The output displays the pod created:</p>
<pre><code>NAME                     READY   STATUS    RESTARTS   AGE
mongo-75f59d57f4-4nd6q   1/1     Running   0          2m4s
</code></pre>
<p>View the Deployment's status:</p>
<pre><code class="language-shell">kubectl get deployment
</code></pre>
<p>The output displays that the Deployment was created:</p>
<pre><code>NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   1/1     1            1           2m21s
</code></pre>
<p>The Deployment automatically manages a ReplicaSet. View the ReplicaSet status using:</p>
<pre><code>kubectl get replicaset
</code></pre>
<p>The output displays that the ReplicaSet was created:</p>
<pre><code>NAME               DESIRED   CURRENT   READY   AGE
mongo-75f59d57f4   1         1         1       3m12s
</code></pre>
<p>Create a Service to expose MongoDB on the network:</p>
<pre><code class="language-shell">kubectl expose deployment mongo --port=27017
</code></pre>
<p>The output of a successful command verifies that the Service was created:</p>
<pre><code>service/mongo created
</code></pre>
<p>Check the Service created:</p>
<pre><code class="language-shell">kubectl get service mongo
</code></pre>
<p>The output displays the service created:</p>
<pre><code>NAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
mongo   ClusterIP   10.96.41.183   &lt;none&gt;        27017/TCP   11s
</code></pre>
<p>Verify that the MongoDB server is running in the Pod, and listening on port 27017:</p>
<p>(Change mongo-75f59d57f4-4nd6q to the name of the Pod)</p>
<pre><code class="language-shell">kubectl get pod mongo-75f59d57f4-4nd6q --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{&quot;\n&quot;}}'
</code></pre>
<p>The output displays the port for MongoDB in that Pod:</p>
<pre><code>27017
</code></pre>
<p>(this is the TCP port allocated to MongoDB on the internet).</p>
<p>Forward a local port to a port on the Pod
kubectl port-forward allows using resource name, such as a pod name, to select a matching pod to port forward to.</p>
<p>(Change mongo-75f59d57f4-4nd6q to the name of the Pod)</p>
<pre><code class="language-shell">kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017
</code></pre>
<p>which is the same as</p>
<pre><code class="language-shell">kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
</code></pre>
<p>or</p>
<pre><code class="language-shell">kubectl port-forward deployment/mongo 28015:27017
</code></pre>
<p>or</p>
<pre><code class="language-shell">kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
</code></pre>
<p>or</p>
<pre><code class="language-shell">kubectl port-forward service/mongo 28015:27017
</code></pre>
<p>Any of the above commands works. The output is similar to this:</p>
<pre><code>Forwarding from 127.0.0.1:28015 -&gt; 27017
Forwarding from [::1]:28015 -&gt; 27017
</code></pre>
<p>Note: kubectl port-forward does not return. To continue with the exercises, you will need to open another terminal.</p>
<p>Install mongodb client package</p>
<pre><code class="language-shell">sudo apt-get install mongodb-clients -y
</code></pre>
<p>Start the MongoDB command line interface:</p>
<pre><code class="language-shell">mongo --port 28015
</code></pre>
<p>At the MongoDB command line prompt, enter the ping command:</p>
<pre><code>db.runCommand( { ping: 1 } )
</code></pre>
<p>A successful ping request returns:</p>
<pre><code>{ ok: 1 }
</code></pre>
<p>Optionally let kubectl choose the local port
If you don't need a specific local port, you can let kubectl choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax:</p>
<pre><code class="language-shell">kubectl port-forward deployment/mongo :27017
</code></pre>
<p>The kubectl tool finds a local port number that is not in use (avoiding low ports numbers, because these might be used by other applications). The output is similar to:</p>
<pre><code>Forwarding from 127.0.0.1:63753 -&gt; 27017
Forwarding from [::1]:63753 -&gt; 27017
</code></pre>
<h2 id="exercise-2---using-kubectl-proxy"><a class="header" href="#exercise-2---using-kubectl-proxy">Exercise 2 - <a href="https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#using-kubectl-proxy">Using kubectl proxy</a></a></h2>
<p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles locating the apiserver and authenticating. Run it like this:</p>
<pre><code class="language-shell">kubectl proxy --port=8080
</code></pre>
<p>See kubectl proxy for more details.</p>
<p>Then you can explore the API with curl, wget, or a browser, replacing localhost with [::1] for IPv6, like so:</p>
<pre><code class="language-shell">curl http://localhost:8080/api/
</code></pre>
<p>The output is similar to this:</p>
<pre><code>{
  &quot;kind&quot;: &quot;APIVersions&quot;,
  &quot;versions&quot;: [
    &quot;v1&quot;
  ],
  &quot;serverAddressByClientCIDRs&quot;: [
    {
      &quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;,
      &quot;serverAddress&quot;: &quot;10.0.1.149:443&quot;
    }
  ]
}
</code></pre>
<p>Without kubectl proxy</p>
<p>Use kubectl describe secret... to get the token for the default service account with grep/cut:</p>
<pre><code class="language-shell">APISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)
SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d &quot; &quot;)

curl $APISERVER/api --header &quot;Authorization: Bearer $TOKEN&quot; --insecure
</code></pre>
<p>The output is similar to this:</p>
<pre><code>{
  &quot;kind&quot;: &quot;APIVersions&quot;,
  &quot;versions&quot;: [
    &quot;v1&quot;
  ],
  &quot;serverAddressByClientCIDRs&quot;: [
    {
      &quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;,
      &quot;serverAddress&quot;: &quot;10.0.1.149:443&quot;
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-ingress-controller"><a class="header" href="#lab-exercises-for-ingress-controller">Lab Exercises for Ingress Controller</a></h1>
<h2 id="exercise-0---deploy-nginx-ingress-controller"><a class="header" href="#exercise-0---deploy-nginx-ingress-controller">Exercise 0 - Deploy NGINX Ingress Controller</a></h2>
<p>Note: <a href="https://kubernetes.github.io/ingress-nginx/deploy">NGINX Ingress Controller</a></p>
<ul>
<li>Install NGINX Ingress Controller</li>
</ul>
<pre><code class="language-shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml
</code></pre>
<ul>
<li>Patch NGINX Ingress Controller with <code>hostNetwork</code> true</li>
</ul>
<pre><code class="language-shell">kubectl patch deployment -n ingress-nginx ingress-nginx-controller --type='json' -p='[{&quot;op&quot;:&quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/hostNetwork&quot;, &quot;value&quot;:true}]'
</code></pre>
<ul>
<li>Verify installation</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch
</code></pre>
<ul>
<li>Check installed version</li>
</ul>
<pre><code class="language-shell">POD_NAMESPACE=ingress-nginx
POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')

kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version
</code></pre>
<p>Output should be similar to</p>
<pre><code>-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v0.46.0
  Build:         6348dde672588d5495f70ec77257c230dc8da134
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.6

-------------------------------------------------------------------------------
</code></pre>
<h2 id="exercise-1---deploy-a-hello-world-app"><a class="header" href="#exercise-1---deploy-a-hello-world-app">Exercise 1 - <a href="https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/#enable-the-ingress-controller">Deploy a hello, world app</a></a></h2>
<p>Create new namespace:</p>
<pre><code class="language-shell">kubectl create namespace app
</code></pre>
<p>Create a Deployment using the following command:</p>
<pre><code class="language-shell">kubectl create deployment -n app web --image=gcr.io/google-samples/hello-app:1.0
</code></pre>
<p>Output:</p>
<pre><code>deployment.apps/web created
</code></pre>
<p>Expose the Deployment:</p>
<pre><code class="language-shell">kubectl expose deployment -n app web --type=ClusterIP --port=8080
</code></pre>
<p>Output:</p>
<pre><code>service/web exposed
</code></pre>
<p>Verify the Service is created and is available on a node port:</p>
<pre><code class="language-shell">kubectl get service -n app web
</code></pre>
<p>Output:</p>
<pre><code>NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web       ClusterIP  10.104.133.249   &lt;none&gt;        8080/TCP         12m
</code></pre>
<p>Create an Ingress resource</p>
<p>The following file is an Ingress resource that sends traffic to your Service via hello-world.info.</p>
<p>Create hello-ingress.yaml from the following file:</p>
<pre><code class="language-shell">cat &gt; hello-ingress.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-ingress
  namespace: app
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: hello-world.info
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web
                port:
                  number: 8080
EOF
</code></pre>
<p>Create the Ingress resource by running the following command:</p>
<pre><code class="language-shell">kubectl apply -f hello-ingress.yaml
</code></pre>
<p>Output:</p>
<pre><code>ingress.networking.k8s.io/example-ingress created
</code></pre>
<p>Verify the IP address is set:</p>
<pre><code class="language-shell">kubectl get ingress -n app
</code></pre>
<p>Note: This can take a couple of minutes.</p>
<pre><code>NAME              CLASS    HOSTS              ADDRESS        PORTS   AGE
hello-ingress     &lt;none&gt;   hello-world.info   10.168.0.202   80      38s
</code></pre>
<p>Add the following line to the bottom of the /etc/hosts file.</p>
<p>Note: The IP address displayed within the ingress list will be the internal IP.</p>
<pre><code>10.168.0.202 hello-world.info
</code></pre>
<pre><code class="language-shell">sudo bash -c 'echo &quot;10.168.0.202 hello-world.info&quot; &gt;&gt; /etc/hosts'
</code></pre>
<p>Verify that the Ingress controller is directing traffic:</p>
<pre><code class="language-shell">curl hello-world.info
</code></pre>
<p>or </p>
<pre><code class="language-shell">curl --header 'Host: hello-world.info' http://10.168.0.202
</code></pre>
<p>Output:</p>
<pre><code>Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre>
<p>Create a second Deployment using the following command:</p>
<pre><code class="language-shell">kubectl create deployment -n app web2 --image=gcr.io/google-samples/hello-app:2.0
</code></pre>
<p>Output:</p>
<pre><code>deployment.apps/web2 created
</code></pre>
<p>Expose the Deployment:</p>
<pre><code class="language-shell">kubectl expose deployment -n app web2 --port=8080 --type=ClusterIP
</code></pre>
<p>Output:</p>
<pre><code>service/web2 exposed
</code></pre>
<p>Edit the existing hello-ingress.yaml and add the following lines:</p>
<pre><code>      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: web2
            port:
              number: 8080
</code></pre>
<p>Apply the changes:</p>
<pre><code class="language-shell">kubectl apply -f hello-ingress.yaml
</code></pre>
<p>Output:</p>
<pre><code>ingress.networking/example-ingress configured
</code></pre>
<p>Test Your Ingress by accessing the 1st version of the Hello World app.</p>
<pre><code class="language-shell">curl hello-world.info
</code></pre>
<p>Output:</p>
<pre><code>Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre>
<p>Access the 2nd version of the Hello World app.</p>
<pre><code class="language-shell">curl hello-world.info/v2
</code></pre>
<p>Output:</p>
<pre><code>Hello, world!
Version: 2.0.0
Hostname: web2-75cd47646f-t8cjk
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-load-balancer"><a class="header" href="#lab-exercises-for-load-balancer">Lab Exercises for Load Balancer</a></h1>
<h2 id="exercise-0---install-bare-metal-load-balancer-service"><a class="header" href="#exercise-0---install-bare-metal-load-balancer-service">Exercise 0 - Install bare-metal Load Balancer service</a></h2>
<p>Note: <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/">Bare-metal ingress controller consideration</a></p>
<p>Note: <a href="https://metallb.universe.tf/concepts/">Metallb concept</a></p>
<ul>
<li>Install <code>metallb</code> as Load Balancer service</li>
</ul>
<pre><code class="language-shell">kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml
</code></pre>
<ul>
<li>Prepare Load Balancer network configuration </li>
</ul>
<pre><code class="language-shell">cat &gt; cm-metallb.yaml &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 10.168.0.50-10.168.0.80
EOF      
</code></pre>
<ul>
<li>Apply Load Balancer configuration</li>
</ul>
<pre><code class="language-shell">kubectl apply -f cm-metallb.yaml
</code></pre>
<ul>
<li>Verify <code>metallb</code> deployment installation</li>
</ul>
<pre><code class="language-shell">kubectl get -n metallb-system pods
kubectl logs -n metallb-system controller-&lt;Tab&gt;
</code></pre>
<ul>
<li>Create new application in specific namespace</li>
</ul>
<pre><code class="language-shell">kubectl create namespace front-web
kubectl create deployment -n front-web web-app --image=registry.k8s:5000/nginx
</code></pre>
<ul>
<li>Scale up application deployment</li>
</ul>
<pre><code class="language-shell">kubectl scale deployment -n front-web web-app --replicas=5
</code></pre>
<ul>
<li>Check deployment status</li>
</ul>
<pre><code class="language-shell">kubectl get deploy,rs,pods -o wide -n front-web
</code></pre>
<ul>
<li>Expose fronted application with service in mode <code>LoadBalancer</code></li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n front-web web-app --port=80 --type=LoadBalancer
</code></pre>
<ul>
<li>Check service status</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n front-web web-app
</code></pre>
<p>Output:</p>
<pre><code>NAME      TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
web-app   LoadBalancer   10.106.220.243   10.168.0.52   80:32586/TCP   7s
</code></pre>
<ul>
<li>Try to connect from remote host</li>
</ul>
<pre><code class="language-shell">curl http://10.168.0.52
</code></pre>
<h2 id="exercise-1---intergrate-metallb-with-nginx-ingress-controller"><a class="header" href="#exercise-1---intergrate-metallb-with-nginx-ingress-controller">Exercise 1 - Intergrate metallb with NGINX Ingress Controller</a></h2>
<p>Note: <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal">Bare-metal consideration</a></p>
<ul>
<li>Patch Ingress Controller and remove <code>hostNetwork</code> capability</li>
</ul>
<pre><code class="language-shell">kubectl patch deployment -n ingress-nginx ingress-nginx-controller --type='json' -p='[{&quot;op&quot;:&quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/spec/hostNetwork&quot;}]'
</code></pre>
<p>Output:</p>
<pre><code>deployment.apps/ingress-nginx-controller patched
</code></pre>
<ul>
<li>Tell NGINX Ingress Controller to use LoadBalancer</li>
</ul>
<pre><code class="language-shell">kubectl patch svc -n ingress-nginx ingress-nginx-controller --type='json' -p='[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;, &quot;value&quot;: &quot;LoadBalancer&quot; }]'
</code></pre>
<p>Output:</p>
<pre><code>service/ingress-nginx-controller patched
</code></pre>
<ul>
<li>Delete LoadBalancer service </li>
</ul>
<pre><code class="language-shell">kubectl delete service -n front-web web-app
</code></pre>
<ul>
<li>Expose once again <code>web-app</code> with <code>ClusterIP</code></li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n front-web web-app --type=ClusterIP --port=80
</code></pre>
<ul>
<li>Check service object</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n front-web
</code></pre>
<ul>
<li>Configure Ingress Controller service</li>
</ul>
<pre><code class="language-shell">cat &gt; front-web.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend-ingress
  namespace: front-web
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: front-web.info
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-app
                port:
                  number: 80
EOF
</code></pre>
<ul>
<li>Apply ingress configuration</li>
</ul>
<pre><code class="language-shell">kubectl apply -f front-web.yaml
</code></pre>
<ul>
<li>Check <code>EXTERNAL-IP</code> for NGINX Ingress Controller</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n ingress-nginx ingress-nginx-controller
</code></pre>
<p>Output:</p>
<pre><code>NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.109.139.127   10.168.0.52   80:31801/TCP,443:31942/TCP   3h16m
</code></pre>
<ul>
<li>Check connection to <code>web-app</code></li>
</ul>
<pre><code class="language-shell">curl -D- http://10.168.0.52 -H 'Host: front-web.info'
</code></pre>
<ul>
<li>Clean up</li>
</ul>
<pre><code class="language-shell">kubectl delete namespace front-web
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-ha-control-plane"><a class="header" href="#lab-exercises-for-ha-control-plane">Lab Exercises for HA Control Plane</a></h1>
<h2 id="exercise-0---make-kubernetes-cluster-fully-high-available"><a class="header" href="#exercise-0---make-kubernetes-cluster-fully-high-available">Exercise 0 - Make Kubernetes cluster fully High-Available</a></h2>
<ul>
<li>On host k8s-host install haproxy package</li>
</ul>
<pre><code class="language-shell">sudo apt-get install haproxy
</code></pre>
<ul>
<li>Edit <code>/etc/haproxy/haproxy.cfg</code> on k8s-host</li>
</ul>
<pre><code>global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# Default ciphers to use on SSL-enabled listening sockets.
	# For more information, see ciphers(1SSL). This list is from:
	#  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
	# An alternative list with additional directives can be obtained from
	#  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy
	ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
	ssl-default-bind-options no-sslv3

defaults
	log	global
	mode	tcp
	option	tcplog
	option	dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http

frontend proxynode
  bind *:80
  bind *:6443
  stats uri /proxystats
  default_backend k8sMasters

backend k8sMasters
  balance roundrobin
  server k8s-master1 10.168.0.101:6443 check
  server k8s-master2 10.168.0.102:6443 check
  server k8s-master3 10.168.0.103:6443 check

listen stats
  bind :9999
  mode http
  stats enable
  stats hide-version
  stats uri /stats
</code></pre>
<ul>
<li>Restart HAproxy service</li>
</ul>
<pre><code class="language-shell">sudo systemctl restart haproxy
</code></pre>
<ul>
<li>Open tunnel and check HA proxy stats</li>
</ul>
<pre><code class="language-shell">ssh -l tux -p &lt;PORT&gt; &lt;LAB_SERVER_IP&gt; -L 9999:10.168.0.1:9999
</code></pre>
<p>Localy on your desktop open in webrowser</p>
<p>http://localhost:9999/stats</p>
<ul>
<li>Now we can switch in all /etc/hosts  on all nodes k8smaster from 10.168.0.101 -&gt; 10.168.0.1</li>
</ul>
<p>Output from /etc/hosts</p>
<pre><code>
...
10.168.0.1 k8smaster
...

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-kubernetes-cni"><a class="header" href="#lab-exercises-for-kubernetes-cni">Lab Exercises for Kubernetes CNI</a></h1>
<p>Notes: <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">Container Network Interface (CNI) Specification</a></p>
<p>Notes: <a href="https://docs.projectcalico.org/reference/architecture/overview">Calico achitecture</a></p>
<h2 id="exercise-0---examine-installed-kubernetes-network-plugin"><a class="header" href="#exercise-0---examine-installed-kubernetes-network-plugin">Exercise 0 - Examine installed Kubernetes Network Plugin</a></h2>
<ul>
<li>SSH on k8s-master1 and check Kubernetes CNI plugin</li>
</ul>
<pre><code class="language-shell">ssh root@master1 cat /etc/cni/net.d/10-calico.conflist
ssh root@master1 cat /etc/cni/net.d/calico-kubeconfig
</code></pre>
<ul>
<li>Check Calico controller and agents</li>
</ul>
<pre><code class="language-shell">kubectl get deployment -n kube-system calico-kube-controllers
kubectl get ds,pod -n kube-system -l k8s-app=calico-node -o wide
</code></pre>
<ul>
<li>Download Calico YAML manifest</li>
</ul>
<pre><code class="language-shell">wget https://docs.projectcalico.org/manifests/calico.yaml
less calico.yaml
</code></pre>
<p>Find configuration section for POD IPs</p>
<pre><code>...
            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            # - name: CALICO_IPV4POOL_CIDR
            #   value: &quot;192.168.0.0/16&quot;
...
</code></pre>
<ul>
<li>Check Calico configuration</li>
</ul>
<pre><code class="language-shell">kubectl get -o yaml ClusterInformation 
kubectl get -o yaml KubeControllersConfiguration
</code></pre>
<ul>
<li>Install Calico client</li>
</ul>
<pre><code class="language-shell">kubectl apply -f https://docs.projectcalico.org/manifests/calicoctl.yaml
</code></pre>
<ul>
<li>Make alias to calicoctl</li>
</ul>
<pre><code class="language-shell">alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl -- /calicoctl&quot;
</code></pre>
<ul>
<li>Check Calico configuration</li>
</ul>
<pre><code class="language-shell">calicoctl get ipPool
calicoctl get node
calicoctl get workloadEndpoint --all-namespaces
calicoctl get profile
</code></pre>
<h2 id="exercise-1---configure-new-ip-pool"><a class="header" href="#exercise-1---configure-new-ip-pool">Exercise 1 - <a href="https://docs.projectcalico.org/getting-started/kubernetes/hardway/configure-ip-pools">Configure new IP pool</a></a></h2>
<ul>
<li>Define new IP pool configuration</li>
</ul>
<pre><code class="language-shell">cat &gt; test-pool.yaml &lt;&lt;EOF
apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: test-pool
spec:
  blockSize: 26
  cidr: 172.19.0.0/16
  ipipMode: Always
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: Never
EOF
</code></pre>
<ul>
<li>Apply <code>test-pool</code> YAML manifest</li>
</ul>
<pre><code class="language-shell">kubectl apply -f test-pool.yaml
</code></pre>
<ul>
<li>Check configuration with kubectl and calicoctl</li>
</ul>
<pre><code class="language-shell">kubectl get ipPool test-pool -o yaml
calicoctl get ippools
</code></pre>
<h2 id="exercise-2---test-network-cni"><a class="header" href="#exercise-2---test-network-cni">Exercise 2 - <a href="https://docs.projectcalico.org/getting-started/kubernetes/hardway/test-networking">Test Network CNI</a></a></h2>
<ul>
<li>Create three <code>busybox</code> instances</li>
</ul>
<pre><code class="language-shell">kubectl create deployment pingtest --image=k8s.gcr.io/busybox --replicas=3 -- sleep 3600
</code></pre>
<p>Check their IP addresses</p>
<pre><code class="language-shell">kubectl get pods --selector=app=pingtest --output=wide
</code></pre>
<p>Output</p>
<pre><code>NAME                        READY   STATUS    RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES
pingtest-64f9cb6b84-qh5zj   1/1     Running   0          9s    192.168.194.84   k8s-worker1   &lt;none&gt;           &lt;none&gt;
pingtest-64f9cb6b84-snl62   1/1     Running   0          9s    192.168.194.85   k8s-worker1   &lt;none&gt;           &lt;none&gt;
pingtest-64f9cb6b84-x46vd   1/1     Running   0          9s    192.168.126.10   k8s-worker2   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Note the IP addresses of the second two pods, then exec into the first one. For example</p>
<pre><code class="language-shell">kubectl exec -ti pingtest-64f9cb6b84-qh5zj -- sh
</code></pre>
<p>From inside the pod, ping the other two pod IP addresses. For example</p>
<pre><code>ping  -c 4 192.168.126.10
</code></pre>
<p>Result</p>
<pre><code>PING 192.168.126.10 (192.168.126.10): 56 data bytes
64 bytes from 192.168.126.10: seq=0 ttl=62 time=1.847 ms
64 bytes from 192.168.126.10: seq=1 ttl=62 time=0.684 ms
64 bytes from 192.168.126.10: seq=2 ttl=62 time=0.488 ms
64 bytes from 192.168.126.10: seq=3 ttl=62 time=0.442 ms

--- 192.168.126.10 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.442/0.865/1.847 ms
</code></pre>
<p>Check routes
From one of the nodes, verify that routes exist to each of the pingtest pods IP addresses. For example</p>
<pre><code class="language-shell">ssh k8s-worker1 ip route get 192.168.126.10
</code></pre>
<p>Result</p>
<pre><code>192.168.126.10 via 10.168.0.202 dev tunl0 src 192.168.194.64 uid 1000 
    cache expires 495sec mtu 1480
</code></pre>
<p>The via 10.168.0.202 in this example indicates the next-hop for this pod IP, which matches the IP address of the node the pod is scheduled on, as expected.</p>
<p>IPAM allocations from different pools
Recall that we created two IP pools, but left one disabled.</p>
<pre><code class="language-shell">calicoctl get ippools -o wide
</code></pre>
<p>Result</p>
<pre><code>NAME                  CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR   
default-ipv4-ippool   192.168.0.0/16   true   Always     Never       false      all()      
test-pool             172.19.0.0/16    true   Always     Never       false      all() 
</code></pre>
<p>Create a pod, explicitly requesting an address from pool2</p>
<pre><code class="language-shell">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: pingtest-pool2
  annotations:
    cni.projectcalico.org/ipv4pools: &quot;[\&quot;test-pool\&quot;]&quot;
spec:
  containers:
  - args:
    - sleep
    - &quot;3600&quot;
    image: k8s.gcr.io/busybox
    imagePullPolicy: Always
    name: pingtest
EOF
</code></pre>
<p>Verify it has an IP address from test-pool</p>
<pre><code class="language-shell">kubectl get pod pingtest-pool2 -o wide
</code></pre>
<p>Result</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE    IP             NODE          NOMINATED NODE   READINESS GATES
pingtest-pool2   1/1     Running   0          3d5h   172.18.126.0   k8s-worker2   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>From one of the original pingtest pods, ping the IP address.</p>
<pre><code class="language-shell">ssh worker1 ping 172.18.126.0 -c 4
ssh master3 ping 172.18.126.0 -c 4
</code></pre>
<p>Result</p>
<pre><code>PING 172.18.126.0 (172.18.126.0) 56(84) bytes of data.
64 bytes from 172.18.126.0: icmp_seq=1 ttl=63 time=0.391 ms
64 bytes from 172.18.126.0: icmp_seq=2 ttl=63 time=0.784 ms
64 bytes from 172.18.126.0: icmp_seq=3 ttl=63 time=0.294 ms
64 bytes from 172.18.126.0: icmp_seq=4 ttl=63 time=0.414 ms

--- 172.18.126.0 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3045ms
rtt min/avg/max/mdev = 0.294/0.470/0.784/0.188 ms

</code></pre>
<p>Clean up</p>
<pre><code>kubectl delete deployments.apps pingtest
kubectl delete pod pingtest-pool2
</code></pre>
<h2 id="optional-0---create-linux-namespaces-with-ip"><a class="header" href="#optional-0---create-linux-namespaces-with-ip">Optional 0 - Create Linux Namespaces with IP</a></h2>
<p>Please <code>k8s-master3</code> as <code>root</code></p>
<ul>
<li>Create Linux network namespace for container1</li>
</ul>
<pre><code class="language-shell">ip netns add container1
</code></pre>
<ul>
<li>Create veth pair network interfaces</li>
</ul>
<pre><code class="language-shell">ip link add veth0c1 type veth peer name veth1c1
</code></pre>
<ul>
<li>Associate the non <code>br-</code> side with the namespace</li>
</ul>
<pre><code class="language-shell">ip link set veth0c1 netns container1
</code></pre>
<ul>
<li>Give namespace-side veth ip addresses</li>
</ul>
<pre><code class="language-shell">ip netns exec container1 ip addr add 172.20.1.5/24 dev veth0c1
</code></pre>
<ul>
<li>Create a bridge device naming it <code>bridge1</code> and set it up</li>
</ul>
<pre><code class="language-shell">ip link add name bridge1 type bridge
</code></pre>
<ul>
<li>Check bridge list</li>
</ul>
<pre><code class="language-shell">brctl show
</code></pre>
<ul>
<li>Turn up the bridge</li>
</ul>
<pre><code class="language-shell">ip link set bridge1 up
</code></pre>
<ul>
<li>Set the bridge veth from the default namespace up</li>
</ul>
<pre><code class="language-shell">ip link set veth1c1 up
</code></pre>
<ul>
<li>Set the veth from the namespace up too</li>
</ul>
<pre><code class="language-shell">ip netns exec container1 ip link set veth0c1 up
</code></pre>
<ul>
<li>Add the veth1c1 interface to the bridge by setting the bridge device as their master</li>
</ul>
<pre><code class="language-shell">ip link set veth1c1 master bridge1
</code></pre>
<ul>
<li>Set the address of the <code>bridge1</code> interface (bridge device) to 172.20.1.1/24 and also set the broadcast address to 172.20.1.255 (the <code>+</code> symbol sets  the host bits to 255).</li>
</ul>
<pre><code class="language-shell">ip addr add 172.20.1.1/24 brd + dev bridge1
</code></pre>
<ul>
<li>Create Linux network namespace for container2</li>
</ul>
<pre><code class="language-shell">ip netns add container2
</code></pre>
<ul>
<li>Create veth pair network interfaces</li>
</ul>
<pre><code class="language-shell">ip link add veth0c2 type veth peer name veth1c2
</code></pre>
<ul>
<li>Associate the non <code>br-</code> side with the namespace</li>
</ul>
<pre><code class="language-shell">ip link set veth0c2 netns container2
</code></pre>
<ul>
<li>Give namespace-side veth ip addresses</li>
</ul>
<pre><code class="language-shell">ip netns exec container2 ip addr add 172.20.1.6/24 dev veth0c2
</code></pre>
<ul>
<li>Set the bridge veth from the default namespace up</li>
</ul>
<pre><code class="language-shell">ip link set veth1c2 up
</code></pre>
<ul>
<li>Set the veth from the namespace up too</li>
</ul>
<pre><code class="language-shell">ip netns exec container2 ip link set veth0c2 up
</code></pre>
<ul>
<li>Add the veth1c2 interface to the bridge by setting the bridge device as their master</li>
</ul>
<pre><code class="language-shell">ip link set veth1c2 master bridge1
</code></pre>
<ul>
<li>Check <code>bridge</code> list</li>
</ul>
<pre><code class="language-shell">brctl show
</code></pre>
<ul>
<li>Ping connection to <code>conainter1</code> and <code>container2</code></li>
</ul>
<pre><code class="language-shell">ping -c 4 172.20.1.1
ping -c 4 172.20.1.5
ping -c 4 172.20.1.6
</code></pre>
<p>============================= PLEASE STOP HERE ===================================</p>
<p>Note: Steps below this line probably will brake your connection to your server</p>
<ul>
<li>Add the physical interface to the bridge</li>
</ul>
<pre><code class="language-shell">ip link set ens4 master bridge1
</code></pre>
<ul>
<li>Add the default gateway in all the network namespace</li>
</ul>
<pre><code class="language-shell">ip netns exec namespace1 ip route add default via 172.20.1.1
</code></pre>
<ul>
<li>Set us up to have responses from the network</li>
</ul>
<pre><code># -t specifies the table to which the commands should be directed to. By default, it's `filter`.
# -A specifies that we're appending a rule to the chain that we tell the name after it.
# -s specifies a source address (with a mask in this case).
# -j specifies the target to jump to (what action to take).
</code></pre>
<pre><code class="language-shell">iptables -t nat -A POSTROUTING -s 172.20.1.0/24 -j MASQUERADE
sysctl -w net.ipv4.ip_forward=1
</code></pre>
<h2 id="optional-1---make-kubernetes-container-namespaces-visible"><a class="header" href="#optional-1---make-kubernetes-container-namespaces-visible">Optional 1 - Make Kubernetes container namespaces visible</a></h2>
<ul>
<li>SSH on <code>k8s-worker2</code></li>
</ul>
<pre><code class="language-shell">ln -s /var/run/docker/netns  /var/run/netns
</code></pre>
<ul>
<li>Then check list of namespaces</li>
</ul>
<pre><code class="language-shell">ip netns list
</code></pre>
<ul>
<li>Try to run some commands inside of containers</li>
</ul>
<pre><code class="language-shell">ip netns exec &lt;NAMESPACE_NAME&gt; ip a 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-exercises-for-network-policy"><a class="header" href="#lab-exercises-for-network-policy">Lab Exercises for Network Policy</a></h1>
<h2 id="exercise-0---testing-network-policy"><a class="header" href="#exercise-0---testing-network-policy">Exercise 0 - Testing Network Policy</a></h2>
<p>Create an nginx deployment and expose it via a service</p>
<pre><code class="language-shell">kubectl create deployment nginx --image=registry.k8s:5000/nginx
</code></pre>
<p>Output:</p>
<pre><code>deployment.apps/nginx created
</code></pre>
<p>Expose the Deployment through a Service called nginx.</p>
<pre><code class="language-shell">kubectl expose deployment nginx --port=8080 --target-port=80
</code></pre>
<p>Output:</p>
<pre><code>service/nginx exposed
</code></pre>
<p>The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named nginx. The nginx Pod and Deployment are found in the default namespace.</p>
<pre><code class="language-shell">kubectl get svc,pod
</code></pre>
<p>Output:</p>
<pre><code>NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes          10.100.0.1    &lt;none&gt;        443/TCP    46m
service/nginx               10.100.0.16   &lt;none&gt;        8080/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
pod/nginx-701339712-e0qfq   1/1           Running       0          35s
</code></pre>
<p>Test the service by accessing it from another Pod
You should be able to access the new nginx service from other Pods. To access the nginx Service from another Pod in the default namespace, start a busybox container:</p>
<pre><code class="language-shell">kubectl run busybox --rm -ti --image=k8s.gcr.io/busybox -- /bin/sh
</code></pre>
<p>In your shell, run the following command:</p>
<pre><code class="language-shell">wget --spider --timeout=1 http://nginx:8080
</code></pre>
<p>Output:</p>
<pre><code>Connecting to nginx:8080 (10.101.252.93:8080)
</code></pre>
<p>Limit access to the nginx service
To limit the access to the nginx service so that only Pods with the label <code>access: enabled</code> can query it, create a NetworkPolicy object as follows:</p>
<pre><code class="language-shell">cat &gt; access-nginx.yaml &lt;&lt; EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: access-nginx
spec:
  podSelector:
    matchLabels:
      app: nginx
  ingress:
  - from:
    - podSelector:
        matchLabels:
          access: enabled
EOF
</code></pre>
<p>The name of a NetworkPolicy object must be a valid DNS subdomain name.</p>
<p>Note: NetworkPolicy includes a podSelector which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label app=nginx. The label was automatically added to the Pod in the nginx Deployment. An empty podSelector selects all pods in the namespace.
Assign the policy to the service
Use kubectl to create a NetworkPolicy from the above access-nginx.yaml file:</p>
<pre><code class="language-shell">kubectl apply -f access-nginx.yaml
</code></pre>
<p>Output:</p>
<pre><code>networkpolicy.networking.k8s.io/access-nginx created
</code></pre>
<p>Test access to the service when access label is not defined
When you attempt to access the nginx Service from a Pod without the correct labels, the request times out:</p>
<pre><code class="language-shell">kubectl run busybox --rm -ti --image=k8s.gcr.io/busybox -- /bin/sh
</code></pre>
<p>In your shell, run the command:</p>
<pre><code class="language-shell">wget --spider --timeout=1 http://nginx:8080
</code></pre>
<p>Output:</p>
<pre><code>Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre>
<p>Define access label and test again
You can create a Pod with the correct labels to see that the request is allowed:</p>
<pre><code class="language-shell">kubectl run busybox --rm -ti --labels=&quot;access=enabled&quot; --image=k8s.gcr.io/busybox -- /bin/sh
</code></pre>
<p>In your shell, run the command:</p>
<pre><code class="language-shell">wget --spider --timeout=1 http://nginx:8080
</code></pre>
<p>Output:</p>
<pre><code>Connecting to nginx (10.100.0.16:8080)
remote file exists
</code></pre>
<h2 id="exercise-1---advanced-network-policy-for-guestbook-with-redis-and-php"><a class="header" href="#exercise-1---advanced-network-policy-for-guestbook-with-redis-and-php">Exercise 1 - <a href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/">Advanced Network Policy for guestbook with Redis and PHP</a></a></h2>
<ul>
<li>Create guestbook namespace</li>
</ul>
<pre><code class="language-shell">kubectl create namespace guestbook
</code></pre>
<ul>
<li>Deny all ingress and egress traffic for namespace <code>guestbook</code></li>
</ul>
<pre><code class="language-shell">cat &gt; guestbook-deny-all.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: guestbook-deny-all
  namespace: guestbook
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
EOF
</code></pre>
<ul>
<li>Apply deny all Network Policy</li>
</ul>
<pre><code class="language-shell">kubectl apply -f guestbook-deny-all.yaml
</code></pre>
<ul>
<li>Allow DNS traffic for all pods in <code>guestbook</code> namespace</li>
</ul>
<pre><code class="language-shell">cat &gt; guestbook-allow-dns.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: guestbook-allow-dns
  namespace: guestbook
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  # allow DNS resolution
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
EOF
</code></pre>
<ul>
<li>Apply DNS Network Policy rules</li>
</ul>
<pre><code class="language-shell">kubectl apply -f guestbook-allow-dns.yaml
</code></pre>
<ul>
<li>Create Redis leader service</li>
</ul>
<pre><code class="language-shell">cat &gt; deploy-redis-leader.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-leader
  namespace: guestbook
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: leader
        tier: backend
    spec:
      containers:
      - name: leader
        image: &quot;registry.k8s:5000/redis:6.0.5&quot;
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF
</code></pre>
<ul>
<li>Apply readis-leader manifest</li>
</ul>
<pre><code class="language-shell">kubectl apply -f deploy-redis-leader.yaml
</code></pre>
<ul>
<li>Verify that Redis leader pod is running</li>
</ul>
<pre><code class="language-shell">kubectl get pods -n guestbook
</code></pre>
<p>Output:</p>
<pre><code>NAME                           READY     STATUS    RESTARTS   AGE
redis-leader-343230949-qfvrq   1/1       Running   0          43s
</code></pre>
<ul>
<li>Check logs from Redis leader</li>
</ul>
<pre><code class="language-shell">kubectl logs -n guestbook deployment/redis-leader
</code></pre>
<p>Output:</p>
<pre><code>1:C 15 Jul 2021 14:23:25.535 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 15 Jul 2021 14:23:25.535 # Redis version=6.0.5, bits=64, commit=00000000, modified=0, pid=1, just started
1:C 15 Jul 2021 14:23:25.535 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
1:M 15 Jul 2021 14:23:25.536 * Running mode=standalone, port=6379.
1:M 15 Jul 2021 14:23:25.537 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
1:M 15 Jul 2021 14:23:25.537 # Server initialized
1:M 15 Jul 2021 14:23:25.537 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
1:M 15 Jul 2021 14:23:25.537 * Ready to accept connections
</code></pre>
<ul>
<li>Expose Redis leader </li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n guestbook redis-leader --port=6379
</code></pre>
<ul>
<li>Verify service</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n guestbook redis-leader
</code></pre>
<p>Output:</p>
<pre><code>NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
redis-leader   ClusterIP   10.110.99.222   &lt;none&gt;        6379/TCP   20s
</code></pre>
<ul>
<li>Define Redis followers</li>
</ul>
<pre><code class="language-shell">cat &gt; deploy-redis-follower.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-follower
  namespace: guestbook
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: follower
        tier: backend
    spec:
      containers:
      - name: follower
        image: gcr.io/google_samples/gb-redis-follower:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF
</code></pre>
<ul>
<li>Create Redis follower deployment</li>
</ul>
<pre><code class="language-shell">kubectl apply -f deploy-redis-follower.yaml
</code></pre>
<ul>
<li>Check logs for Redis follower pods</li>
</ul>
<pre><code class="language-shell">kubectl logs -n guestbook redis-follower-&lt;Tab&gt;
</code></pre>
<p>Output:</p>
<pre><code>10:S 27 Jul 2021 10:15:23.836 * Ready to accept connections
10:S 27 Jul 2021 10:15:23.836 * Connecting to MASTER redis-leader:6379
10:S 27 Jul 2021 10:15:23.843 * MASTER &lt;-&gt; REPLICA sync started
10:S 27 Jul 2021 10:16:24.068 # Timeout connecting to the MASTER...
10:S 27 Jul 2021 10:16:24.068 * Connecting to MASTER redis-leader:6379
10:S 27 Jul 2021 10:16:24.069 * MASTER &lt;-&gt; REPLICA sync started
10:S 27 Jul 2021 10:17:25.313 # Timeout connecting to the MASTER...
</code></pre>
<p>Wait and look for</p>
<pre><code># Timeout connecting to the MASTER...
</code></pre>
<ul>
<li>Allow traffic from Redis follower to Redis leader</li>
</ul>
<pre><code class="language-shell">cat &gt; guestbook-allow-redis-sync.yaml&lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: guestobook-allow-redis-sync
  namespace: guestbook
spec:
  podSelector:
    matchLabels:
      app: redis
      role: leader
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: guestbook
    - podSelector:
        matchLabels:
          app: redis
          role: follower
    ports:
    - protocol: TCP
      port: 6379
EOF
</code></pre>
<ul>
<li>Check once again logs from Redis followers</li>
</ul>
<pre><code class="language-shell">kubectl logs -n guestbook redis-follower-&lt;Tab&gt;
</code></pre>
<p>Output:</p>
<pre><code>10:S 15 Jul 2021 14:53:07.966 * Connecting to MASTER redis-leader:6379
10:S 15 Jul 2021 14:53:07.970 * MASTER &lt;-&gt; REPLICA sync started
</code></pre>
<ul>
<li>Expose Redis follower service</li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n guestbook redis-follower --port=6379
</code></pre>
<ul>
<li>Check list of services</li>
</ul>
<pre><code class="language-shell">kubectl get svc -n guestbook
</code></pre>
<p>Output:</p>
<pre><code>NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
redis-follower   ClusterIP   10.98.8.13      &lt;none&gt;        6379/TCP   53s
redis-leader     ClusterIP   10.110.99.222   &lt;none&gt;        6379/TCP   49m
</code></pre>
<ul>
<li>Define frontend YAML manifest application</li>
</ul>
<pre><code class="language-shell">cat &gt; deploy-frontend.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: guestbook
spec:
  replicas: 3
  selector:
    matchLabels:
        app: guestbook
        tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v5
        env:
        - name: GET_HOSTS_FROM
          value: &quot;dns&quot;
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80
EOF
</code></pre>
<ul>
<li>Apply YAML deployment </li>
</ul>
<pre><code class="language-shell">kubectl apply -f deploy-frontend.yaml
</code></pre>
<ul>
<li>Check pods status for frontend deployment</li>
</ul>
<pre><code class="language-shell">kubectl get pods -l app=guestbook -l tier=frontend -n guestbook
</code></pre>
<p>Output:</p>
<pre><code>NAME                        READY   STATUS    RESTARTS   AGE
frontend-85595f5bf9-2lmnb   1/1     Running   0          7m20s
frontend-85595f5bf9-bv9sx   1/1     Running   0          7m19s
frontend-85595f5bf9-p974r   1/1     Running   0          7m19s
</code></pre>
<ul>
<li>Expose frontend application with LoadBalancer</li>
</ul>
<pre><code class="language-shell">kubectl expose deployment -n guestbook frontend --port=80 --type=LoadBalancer
</code></pre>
<ul>
<li>Check service </li>
</ul>
<pre><code class="language-shell">kubectl get svc -n guestbook
</code></pre>
<p>Output:</p>
<pre><code>NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
frontend         LoadBalancer   10.106.92.142   10.168.0.50   80:30547/TCP   7m22s
redis-follower   ClusterIP      10.98.8.13      &lt;none&gt;        6379/TCP       15h
redis-leader     ClusterIP      10.110.99.222   &lt;none&gt;        6379/TCP       16h
</code></pre>
<ul>
<li>Use <code>EXTERNALIP</code> and try to connect </li>
</ul>
<pre><code class="language-shell">EXTERNALIP=`kubectl get svc -n guestbook frontend -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;`
curl --connect-timeout 5 http://$EXTERNALIP
</code></pre>
<p>Output:</p>
<pre><code>curl: (28) Connection timed out after 5001 milliseconds
</code></pre>
<ul>
<li>Prepare Network Policy to allow connection to frontend</li>
</ul>
<pre><code class="language-shell">cat &gt; allow-frontend.yaml &lt;&lt;EOF
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-frontend
  namespace: guestbook
spec:
  podSelector:
    matchLabels:
      app: guestbook
      tier: frontend
  ingress:
  - ports:
    - port: 80
EOF
</code></pre>
<ul>
<li>Apply frontend Network Policy</li>
</ul>
<pre><code class="language-shell">kubectl apply -f allow-frontend.yaml
</code></pre>
<ul>
<li>Use <code>EXTERNALIP</code> and try to connect once again</li>
</ul>
<pre><code class="language-shell">EXTERNALIP=`kubectl get svc -n guestbook frontend -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;`
curl --connect-timeout 5 http://$EXTERNALIP
</code></pre>
<p>Output:</p>
<pre><code>&lt;html ng-app=&quot;redis&quot;&gt;
  &lt;head&gt;
    &lt;title&gt;Guestbook&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css&quot;&gt;
    &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;controllers.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/angular-ui-bootstrap/0.13.0/ui-bootstrap-tpls.js&quot;&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body ng-controller=&quot;RedisCtrl&quot;&gt;
    &lt;div style=&quot;width: 50%; margin-left: 20px&quot;&gt;
      &lt;h2&gt;Guestbook&lt;/h2&gt;
    &lt;form&gt;
    &lt;fieldset&gt;
    &lt;input ng-model=&quot;msg&quot; placeholder=&quot;Messages&quot; class=&quot;form-control&quot; type=&quot;text&quot; name=&quot;input&quot;&gt;&lt;br&gt;
    &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; ng-click=&quot;controller.onRedis()&quot;&gt;Submit&lt;/button&gt;
    &lt;/fieldset&gt;
    &lt;/form&gt;
    &lt;div&gt;
      &lt;div ng-repeat=&quot;msg in messages track by $index&quot;&gt;
        {{msg}}
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<ul>
<li>Last step is to allow connection from frontend application to redis instances</li>
</ul>
<pre><code class="language-shell">cat &gt; allow-fronted-redis.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-redis
  namespace: guestbook
spec:
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: guestbook
    ports:
    - port: 6380
      protocol: TCP
    - port: 6379
      protocol: TCP
  podSelector:
    matchLabels:
      app: redis
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-redis-frontend
  namespace: guestbook
spec:
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: redis
  - ports:
    - port: 6380
      protocol: TCP
    - port: 6379
      protocol: TCP
  podSelector:
    matchLabels:
      app: guestbook
  policyTypes:
  - Egress
EOF
</code></pre>
<ul>
<li>Apply Network Policy</li>
</ul>
<pre><code class="language-shell">kubectl apply -f allow-fronted-redis.yaml
</code></pre>
<ul>
<li>Check connection using Webrowser</li>
</ul>
<pre><code>http://&lt;EXTERNALIP&gt;
</code></pre>
<p>Note: This can be done by using graphical interface to server or by SSH port forwarding from your lab environment to your desktop e.g.</p>
<pre><code>ssh -L 8080:EXTERNALIP:80 -l tux -p &lt;PORT&gt; &lt;LAB_SERVER_IP&gt;
</code></pre>
<p>Then localy on desktop open </p>
<p>http://localhost:8080</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
